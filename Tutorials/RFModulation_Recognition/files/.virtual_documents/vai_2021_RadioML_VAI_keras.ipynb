





from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
import keras
from tensorflow import Tensor
print("Tensorflow version is ", tf.__version__)
print('Keras version      : ',keras.__version__)
import numpy as np
import os, sys
import cv2
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd
import h5py as h5
from sklearn.metrics import classification_report, confusion_matrix
import random
import time
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D, AveragePooling2D
from tensorflow.keras.layers import  Dropout, Activation, GlobalAveragePooling1D, ZeroPadding2D, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam, SGD, Adadelta
from tensorflow.keras.layers import Reshape, Dense, Flatten, Add
from tensorflow.keras.activations import relu
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, History, TensorBoard
from tensorflow.keras import backend as K
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Sequential

from random import shuffle
import gc
import glob
from sklearn.model_selection import train_test_split
from pathlib import Path
import pickle
from progressbar import ProgressBar
import matplotlib.pyplot as plt





# !tar -xvf RADIOML_2021_07_INT8.tar.gz
!ls /datasets/RADIOML_2021_07_INT8/








#Note this is needed to aviod a tensorFlow memory issue
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

data_file = '/datasets/RADIOML_2021_07_INT8/RADIOML_2021_07_INT8.hdf5'
file_handle = h5.File(data_file,'r+')

myData = file_handle['X'][:]  #1024x2 samples 
myMods = file_handle['Y'][:]  #mods 
mySNRs = file_handle['Z'][:]  #snrs  

print(np.shape(myData))
print(np.shape(myMods))
print(np.shape(mySNRs))
file_handle.close()

np.random.seed(0)





snrs = list(np.unique(mySNRs.T[0]))  
print(snrs)





mods = ["OOK","4ASK","8ASK",
        "BPSK","QPSK","8PSK","16PSK","32PSK",
        "16APSK","32APSK","64APSK","128APSK",
        "16QAM","32QAM","64QAM","128QAM","256QAM",
        "AM-SSB-WC","AM-SSB-SC","AM-DSB-WC","AM-DSB-SC","FM",
        "GMSK","OQPSK","BFSK","4FSK","8FSK"]

num_classes = np.shape(mods)[0]
print("The number of classes is ", num_classes)






#turn off warning about more than 10 figures plotted
plt.rcParams.update({'figure.max_open_warning': 0})

def my_range(start, end, step):
    while start <= end:
        yield start
        start += step

size = np.size(myData, axis = 0)
step = size//27

for x in my_range(100000, (size-1), step):
  plt.figure()
  plt.suptitle( mods[np.argmax(myMods[x])])
  plt.plot(myData[x,:,0])
  plt.plot(myData[x,:,1])





myData = myData.reshape(myData.shape[0], 1024, 1, 2) 





X_train ,X_test ,Y_train ,Y_test, Z_train, Z_test = train_test_split(myData, myMods, mySNRs, test_size=0.2, random_state=0)
X_val, X_test, Y_val, Y_test, Z_val, Z_test = train_test_split(X_test, Y_test, Z_test, test_size=0.5, random_state=0)
print (np.shape(X_test))
print (np.shape(Y_test))
print (np.shape(Z_test))
print (np.shape(X_train))
print (np.shape(Y_train))
print (np.shape(Z_train))
del myData, myMods, mySNRs


def best_snr_data(X, Y, Z):
    best_snr_indices = np.where(np.any(Z == 30, axis=1))
    return X[best_snr_indices], Y[best_snr_indices]


X_best_test, Y_best_test = best_snr_data(X_test, Y_test, Z_test)
print(np.shape(X_best_test))
print(np.shape(Y_best_test))


def usable_snr_data(X, Y, Z):
    best_snr_indices = np.where(np.any(Z >= 6, axis=1))
    return X[best_snr_indices], Y[best_snr_indices]


X_usable_test, Y_usable_test = usable_snr_data(X_test, Y_test, Z_test)
print(np.shape(X_usable_test))
print(np.shape(Y_usable_test))





def get_balanced_snr_samples(X, Y, Z, samples_per_snr=30, seed=None):
    """
    Returns X and Y containing `samples_per_snr` randomly selected samples
    for each unique SNR value in Z.

    Parameters:
    - X: Input data (numpy array)
    - Y: Labels corresponding to X
    - Z: SNR values for each sample
    - samples_per_snr: Number of samples to select for each SNR value
    - seed: Optional random seed for reproducibility

    Returns:
    - Tuple of (X_filtered, Y_filtered)
    """
    if seed is not None:
        np.random.seed(seed)

    unique_snrs = np.unique(Z)
    selected_X = []
    selected_Y = []

    for snr in unique_snrs:
        indices = np.where(Z == snr)[0]
        if len(indices) < samples_per_snr:
            raise ValueError(f"Not enough samples for SNR {snr}: requested {samples_per_snr}, but found {len(indices)}")
        
        chosen_indices = np.random.choice(indices, samples_per_snr, replace=False)
        selected_X.append(X[chosen_indices])
        selected_Y.append(Y[chosen_indices])

    return np.concatenate(selected_X), np.concatenate(selected_Y)









input_shp = list(X_train.shape[1:])
print("Dataset Shape={0} CNN Model Input layer={1}".format(X_train.shape, input_shp))
classes = mods





def resnet_block(input_data, in_filters, out_filters, conv_size):
  x = Conv2D(in_filters, conv_size, activation=None, padding='same')(input_data)
  x = BatchNormalization()(x)
  x = Add()([x, input_data])
  x = Activation('relu')(x)
  x = Conv2D(out_filters, conv_size, activation=None, padding='same')(x) 
  x = BatchNormalization()(x)
  x = MaxPooling2D(2, strides = (2,1), padding = 'same') (x)
  return x







num_resnet_blocks = 5
kernel_size = 5,1

rf_input = Input(shape=input_shp, name = 'rf_input')

x = Conv2D(16, (kernel_size), activation=None, padding='same')(rf_input)
x = BatchNormalization()(x)
x = Activation('relu')(x)

in_filters = 16
out_filters = 32
for i in range(num_resnet_blocks):
    if (i == num_resnet_blocks-1):
        out_filters = num_classes
    x = resnet_block(x, in_filters, out_filters, kernel_size)
    in_filters = in_filters * 2
    out_filters = out_filters * 2

flatten = Flatten()(x)
dropout_1 = Dropout(0.5)(flatten)
dense_1 = Dense(num_classes, activation='relu')(dropout_1)        
softmax = Activation('softmax', name = 'softmax')(dense_1)

optimizer= Adam(learning_rate=0.00060)
model = keras.Model(rf_input, softmax)
model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

print(model.summary())


rf_input = Input(shape=input_shp, name = 'rf_input')

x = Conv2D(filters=16, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(rf_input)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=64, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = BatchNormalization()(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(4, 1))(x)
x = Flatten()(x)

rf_output = Dense(27, 
                  kernel_initializer='glorot_uniform', 
                  activation='softmax')(x)

optimizer= Adam(learning_rate=0.00060)
model_0 = keras.Model(rf_input, rf_output)
model_0.compile(loss='categorical_crossentropy', metrics=["accuracy"])

print(model_0.summary())


rf_input_1 = Input(shape=input_shp, name = 'rf_input_1')

x = Conv2D(filters=16, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(rf_input_1)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = BatchNormalization()(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=64, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = BatchNormalization()(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = GlobalAveragePooling2D()(x)
x = Dense(16, 
          kernel_initializer='he_uniform', 
          activation='relu')(x)

rf_output_1 = Dense(27, 
                  kernel_initializer='glorot_uniform', 
                  activation='softmax')(x)

optimizer= Adam(learning_rate=0.00060)
model_1 = keras.Model(rf_input_1, rf_output_1)
model_1.compile(loss='categorical_crossentropy', metrics=["accuracy"])

print(model_1.summary())


rf_input_2 = Input(shape=input_shp, name = 'rf_input')

x = Conv2D(filters=32, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(rf_input_2)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=4, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=64, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='relu', 
           padding='same')(x)
x = BatchNormalization()(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(2, 1))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(16, 
          kernel_initializer='he_uniform', 
          activation='relu')(x)
x = Dense(16, 
          kernel_initializer='he_uniform', 
          activation='relu')(x)
x = Dense(16, 
          kernel_initializer='he_uniform', 
          activation='relu')(x)

rf_output_2 = Dense(27, 
                  kernel_initializer='glorot_uniform', 
                  activation='softmax')(x)

optimizer= Adam(learning_rate=0.00060)
model_2 = keras.Model(rf_input_2, rf_output_2)
model_2.compile(loss='categorical_crossentropy', metrics=["accuracy"])

print(model_2.summary())


rf_input_3 = Input(shape=input_shp, name = 'rf_input')

x = Conv2D(filters=64, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(rf_input_3)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = BatchNormalization()(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=16, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(4, 1))(x)
x = Conv2D(filters=8, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=64, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = GlobalAveragePooling2D()(x)
x = Dense(16, 
          kernel_initializer='he_uniform', 
          activation='selu')(x)

rf_output_3 = Dense(27, 
                  kernel_initializer='glorot_uniform', 
                  activation='softmax')(x)

optimizer= Adam(learning_rate=0.00060)
model_3 = keras.Model(rf_input_3, rf_output_3)
model_3.compile(loss='categorical_crossentropy', metrics=["accuracy"])

print(model_3.summary())


rf_input_4 = Input(shape=input_shp, name = 'rf_input')

x = Conv2D(filters=8, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(rf_input_4)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(4, 1))(x)
x = Conv2D(filters=64, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = BatchNormalization()(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(4, 1), 
                     strides=(4, 1))(x)
x = Conv2D(filters=8, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=32, 
           kernel_size=(2, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Conv2D(filters=16, 
           kernel_size=(1, 1),      
           strides=(1, 1), 
           kernel_initializer='he_uniform',
           activation='selu', 
           padding='same')(x)
x = AveragePooling2D(pool_size=(2, 1), 
                     strides=(2, 1))(x)
x = Flatten()(x)

rf_output_4 = Dense(27, 
                  kernel_initializer='glorot_uniform', 
                  activation='softmax')(x)

optimizer= Adam(learning_rate=0.00060)
model_4 = keras.Model(rf_input_4, rf_output_4)
model_4.compile(loss='categorical_crossentropy', metrics=["accuracy"])

print(model_4.summary())





predict = model.predict(X_test[0:1])
print(predict)








nb_epoch = 100     # number of epochs to train on
batch_size = 1024  # training batch size





### Callback
checkpoint_dir = 'resnet_checkpoints'
#os.mkdir(checkpoint_dir)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_dir + '/model_2_relu_100ep.h5', 
                                                 verbose = 1,
                                                 save_best_only=True, 
                                                 save_weights_only=False,
                                                 mode='auto')
# tb_callback = TensorBoard(log_dir=os.path.join('learner_logs', 'model_0'),
#                           histogram_freq=1,
#                           profile_batch=0)

callbacks = [cp_callback]





# history = model.fit(
#     X_train,
#     Y_train,
#     batch_size=batch_size,
#     epochs=100,
#     verbose=1,
#     validation_data=(X_test, Y_test),
#     callbacks = [ cp_callback ]
#     )



model_0.fit(
    X_train,
    Y_train,
    epochs=nb_epoch,
    validation_data=(X_val, Y_val),
    verbose=1,
    callbacks=callbacks
)

K.clear_session()
gc.collect()

# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_dir + '/model_1_relu.h5', 
#                                                  verbose = 1,
#                                                  save_best_only=True, 
#                                                  save_weights_only=False,
#                                                  mode='auto')

# tb_callback = TensorBoard(log_dir=os.path.join('learner_logs', 'model_1'),
#                           histogram_freq=1,
#                           profile_batch=0)


model_1.fit(
    X_train,
    Y_train,
    epochs=nb_epoch,
    validation_data=(X_val, Y_val),
    verbose=1,
    callbacks=callbacks
)

K.clear_session()
gc.collect()


model_2.fit(
    X_train,
    Y_train,
    epochs=nb_epoch,
    validation_data=(X_val, Y_val),
    verbose=1,
    callbacks=callbacks
)

K.clear_session()
gc.collect()


model_3.fit(
    X_train,
    Y_train,
    epochs=nb_epoch,
    validation_data=(X_val, Y_val),
    verbose=1,
    callbacks=callbacks
)

K.clear_session()
gc.collect()

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_dir + '/model_4.h5', 
                                                 verbose = 1,
                                                 save_best_only=True, 
                                                 save_weights_only=False,
                                                 mode='auto')

tb_callback = TensorBoard(log_dir=os.path.join('learner_logs', 'model_4'),
                          histogram_freq=1,
                          profile_batch=0)


model_4.fit(
    X_train,
    Y_train,
    epochs=nb_epoch,
    validation_data=(X_val, Y_val),
    verbose=1,
    callbacks=callbacks
)

K.clear_session()
gc.collect()





# best_checkpoint = checkpoint_dir + '/best_checkpoint.h5'
# model.load_weights(best_checkpoint)
# !mkdir -p fp_model
# model.save ('fp_model/resnet_fp_model.h5')


model_0 = tf.keras.models.load_model('resnet_checkpoints/model_0_relu.h5')


model_1 = tf.keras.models.load_model('resnet_checkpoints/model_1_relu_100ep.h5')


model_2 = tf.keras.models.load_model('resnet_checkpoints/model_2_relu_100ep.h5')





# Show simple version of performance
score = model_0.evaluate(X_test, Y_test,  verbose=0, batch_size=batch_size)
print(score)


score = model_0.evaluate(X_best_test, Y_best_test,  verbose=0, batch_size=batch_size)
print(score)


score = model_1.evaluate(X_test, Y_test,  verbose=0, batch_size=batch_size)
print(score)


score = model_1.evaluate(X_best_test, Y_best_test,  verbose=0, batch_size=batch_size)
print(score)


score = model_2.evaluate(X_test, Y_test,  verbose=0, batch_size=batch_size)
print(score)


score = model_2.evaluate(X_best_test, Y_best_test,  verbose=0, batch_size=batch_size)
print(score)








def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):
    plt.figure(figsize = (15,10))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    label_len = np.shape(labels)[0]
    tick_marks = np.arange(label_len)
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.tight_layout()    
    plt.ylabel('True label')
    plt.xlabel('Predicted label')


# Plot confusion matrix
test_Y_hat = model_0.predict(X_test, batch_size=batch_size)
conf = np.zeros([num_classes,num_classes])
confnorm = np.zeros([num_classes,num_classes])
for i in range(0,X_test.shape[0]):
    j = list(Y_test[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,num_classes):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])


plot_confusion_matrix(confnorm, labels=classes)


test_Y_hat = model_1.predict(X_test, batch_size=batch_size)
conf = np.zeros([num_classes,num_classes])
confnorm = np.zeros([num_classes,num_classes])
for i in range(0,X_test.shape[0]):
    j = list(Y_test[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,num_classes):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])


plot_confusion_matrix(confnorm, labels=classes)


test_Y_hat = model_2.predict(X_test, batch_size=batch_size)
conf = np.zeros([num_classes,num_classes])
confnorm = np.zeros([num_classes,num_classes])
for i in range(0,X_test.shape[0]):
    j = list(Y_test[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,num_classes):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])


plot_confusion_matrix(confnorm, labels=classes)





Y_pred = model_0.predict(X_test,batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis = 1)
y_actual = np.argmax(Y_test, axis = 1)
classificationreport_fp = classification_report(y_actual,y_pred, target_names=mods)
print(classificationreport_fp)


Y_pred = model_1.predict(X_test,batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis = 1)
y_actual = np.argmax(Y_test, axis = 1)
classificationreport_fp = classification_report(y_actual,y_pred, target_names=mods)
print(classificationreport_fp)


Y_pred = model_2.predict(X_test,batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis = 1)
y_actual = np.argmax(Y_test, axis = 1)
classificationreport_fp = classification_report(y_actual,y_pred, target_names=mods)
print(classificationreport_fp)








batchsize = 1024
progress = ProgressBar()
snrlist = np.unique(Z_test)
acc_snr_arr = []

# interate over SNRs
for snr in progress(snrlist):
    acc_arr = []
    i_SNR = np.where(Z_test==snr)
    X_SNR = X_test[i_SNR[0],:,:]
    Y_SNR = Y_test[i_SNR[0],:]
    X_SNR_len = np.shape(X_SNR)[0]
    total_batches = int(X_SNR_len/batchsize)
    
    for i in (range(0, total_batches)):
        x_batch, y_batch = X_SNR[i*batchsize:i*batchsize+batchsize], Y_SNR[i*batchsize:i*batchsize+batchsize]
        
        # model prediction
        pred = model_0.predict(x_batch)
        
        #Pediction values are onehote, corresponding to indices representing different modulation types
        pred_ind = np.argmax(pred, axis=1)
        expected_ind = np.argmax(y_batch, axis=1)
        matches  = sum(np.equal(pred_ind, expected_ind))
        acc      = matches/batchsize
        acc_arr.append(acc)

    # Average the per-batch accuracy values
    accuracy = np.mean(acc_arr)
    acc_snr_arr.append(accuracy)
    print("SNR: ", snr, "accuracy", accuracy)


plt.figure(figsize=(1,1))
plt.show()
fig= plt.figure(figsize=(10,8))
plt.plot(snrlist, acc_snr_arr, 'bo-', label='accuracy')
plt.ylabel('Accuracy')
plt.xlabel('SNR')
plt.title("Accuracy vs, SNR for Floating Point Model")
plt.legend()
plt.axis([-22, 32, 0, 1.0])
plt.grid()


batchsize = 1024
progress = ProgressBar()
snrlist = np.unique(Z_test)
acc_snr_arr = []

# interate over SNRs
for snr in progress(snrlist):
    acc_arr = []
    i_SNR = np.where(Z_test==snr)
    X_SNR = X_test[i_SNR[0],:,:]
    Y_SNR = Y_test[i_SNR[0],:]
    X_SNR_len = np.shape(X_SNR)[0]
    total_batches = int(X_SNR_len/batchsize)
    
    for i in (range(0, total_batches)):
        x_batch, y_batch = X_SNR[i*batchsize:i*batchsize+batchsize], Y_SNR[i*batchsize:i*batchsize+batchsize]
        
        # model prediction
        pred = model_1.predict(x_batch)
        
        #Pediction values are onehote, corresponding to indices representing different modulation types
        pred_ind = np.argmax(pred, axis=1)
        expected_ind = np.argmax(y_batch, axis=1)
        matches  = sum(np.equal(pred_ind, expected_ind))
        acc      = matches/batchsize
        acc_arr.append(acc)

    # Average the per-batch accuracy values
    accuracy = np.mean(acc_arr)
    acc_snr_arr.append(accuracy)
    print("SNR: ", snr, "accuracy", accuracy)


plt.figure(figsize=(1,1))
plt.show()
fig= plt.figure(figsize=(10,8))
plt.plot(snrlist, acc_snr_arr, 'bo-', label='accuracy')
plt.ylabel('Accuracy')
plt.xlabel('SNR')
plt.title("Accuracy vs, SNR for Floating Point Model")
plt.legend()
plt.axis([-22, 32, 0, 1.0])
plt.grid()


batchsize = 1024
progress = ProgressBar()
snrlist = np.unique(Z_test)
acc_snr_arr = []

# interate over SNRs
for snr in progress(snrlist):
    acc_arr = []
    i_SNR = np.where(Z_test==snr)
    X_SNR = X_test[i_SNR[0],:,:]
    Y_SNR = Y_test[i_SNR[0],:]
    X_SNR_len = np.shape(X_SNR)[0]
    total_batches = int(X_SNR_len/batchsize)
    
    for i in (range(0, total_batches)):
        x_batch, y_batch = X_SNR[i*batchsize:i*batchsize+batchsize], Y_SNR[i*batchsize:i*batchsize+batchsize]
        
        # model prediction
        pred = model_2.predict(x_batch)
        
        #Pediction values are onehote, corresponding to indices representing different modulation types
        pred_ind = np.argmax(pred, axis=1)
        expected_ind = np.argmax(y_batch, axis=1)
        matches  = sum(np.equal(pred_ind, expected_ind))
        acc      = matches/batchsize
        acc_arr.append(acc)

    # Average the per-batch accuracy values
    accuracy = np.mean(acc_arr)
    acc_snr_arr.append(accuracy)
    print("SNR: ", snr, "accuracy", accuracy)


plt.figure(figsize=(1,1))
plt.show()
fig= plt.figure(figsize=(10,8))
plt.plot(snrlist, acc_snr_arr, 'bo-', label='accuracy')
plt.ylabel('Accuracy')
plt.xlabel('SNR')
plt.title("Accuracy vs, SNR for Floating Point Model")
plt.legend()
plt.axis([-22, 32, 0, 1.0])
plt.grid()











X_test_calib, Y_test_calib = get_balanced_snr_samples(X_test, Y_test, Z_test, samples_per_snr=40, seed=0)

print(np.shape(X_test_calib))
print(np.shape(Y_test_calib))


model_0 = tf.keras.models.load_model("resnet_checkpoints/model_0_relu.h5")


model_1 = tf.keras.models.load_model("resnet_checkpoints/model_1_relu_100ep_fftv2.h5")


model_2 = tf.keras.models.load_model("resnet_checkpoints/model_2_relu_100ep.h5")


from tensorflow_model_optimization.quantization.keras import vitis_quantize
# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
quantizer = vitis_quantize.VitisQuantizer(model_0)
quantized_model = quantizer.quantize_model(calib_dataset = X_test[1:1000])

# Save the model
!mkdir -p quantize_results
quantized_model.save('resnet_checkpoints/model_0_relu_quantized_nofft.h5')
!ls -l quantize_results


from tensorflow_model_optimization.quantization.keras import vitis_quantize
quantizer = vitis_quantize.VitisQuantizer(model_1)
quantized_model = quantizer.quantize_model(calib_dataset = X_test[1:1000])

# Save the model
!mkdir -p quantize_results
quantized_model.save('resnet_checkpoints/model_1_relu_100ep_nofft.h5')
!ls -l quantize_results


from tensorflow_model_optimization.quantization.keras import vitis_quantize
quantizer = vitis_quantize.VitisQuantizer(model_2)
quantized_model = quantizer.quantize_model(calib_dataset = X_test[1:1000])

# Save the model
!mkdir -p quantize_results
quantized_model.save('resnet_checkpoints/model_2_relu_100ep_quantized_nofft.h5')
!ls -l quantize_results








# load quantized model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
  q_model = tf.keras.models.load_model('resnet_checkpoints/model_0_relu_quantized_nofft.h5', compile=False)

q_model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

score = q_model.evaluate(X_test, Y_test,  verbose=0, batch_size=1024)
print(score)


# load quantized model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
  q_model = tf.keras.models.load_model('resnet_checkpoints/model_0_relu_quantized_nofft.h5', compile=False)

q_model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

score = q_model.evaluate(X_best_test, Y_best_test,  verbose=0, batch_size=1024)
print(score)


# load quantized model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
  q_model = tf.keras.models.load_model('resnet_checkpoints/model_1_relu_100ep_nofft.h5', compile=False)

q_model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

score = q_model.evaluate(X_test, Y_test,  verbose=0, batch_size=1024)
print(score)


# load quantized model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
  q_model = tf.keras.models.load_model('resnet_checkpoints/model_1_relu_100ep_nofft.h5', compile=False)

q_model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

score = q_model.evaluate(X_best_test, Y_best_test,  verbose=0, batch_size=1024)
print(score)


# load quantized model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
  q_model = tf.keras.models.load_model('resnet_checkpoints/model_2_relu_100ep_quantized_nofft.h5', compile=False)

q_model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

score = q_model.evaluate(X_test, Y_test,  verbose=0, batch_size=1024)
print(score)


# load quantized model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
  q_model = tf.keras.models.load_model('resnet_checkpoints/model_2_relu_100ep_quantized_nofft.h5', compile=False)

q_model.compile(loss='categorical_crossentropy', metrics=["accuracy"])

score = q_model.evaluate(X_best_test, Y_best_test,  verbose=0, batch_size=1024)
print(score)


q_model.fit(
    X_train,
    Y_train,
    epochs=100,
    validation_data=(X_test, Y_test),
    verbose=1,
    callbacks=
    [
        EarlyStopping(
        monitor='val_loss',
        patience=5,
        verbose=1)     
    ])


q_model.save('resnet_checkpoints/model_0_quantized.h5')








with vitis_quantize.quantize_scope():
    q_model = tf.keras.models.load_model("resnet_checkpoints/model_1_relu_100ep_fftv2.h5")

Y_pred = q_model.predict(X_usable_test,batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis = 1)
y_actual = np.argmax(Y_usable_test, axis = 1)
classificationreport_int8 = classification_report(y_actual,y_pred, target_names=mods)
print(classificationreport_int8)


# Plot confusion matrix
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
    q_model = tf.keras.models.load_model("resnet_checkpoints/model_1_quantized.h5")
test_Y_hat = q_model.predict(X_test, batch_size=1024)
conf = np.zeros([num_classes,num_classes])
confnorm = np.zeros([num_classes,num_classes])
for i in range(0,X_test.shape[0]):
    j = list(Y_test[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,num_classes):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])


plot_confusion_matrix(confnorm, labels=classes)


# Plot confusion matrix
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
    q_model = tf.keras.models.load_model("resnet_checkpoints/model_1_relu_100ep_fftv2.h5")
test_Y_hat = q_model.predict(X_usable_test, batch_size=1024)
conf = np.zeros([num_classes,num_classes])
confnorm = np.zeros([num_classes,num_classes])
for i in range(0,X_usable_test.shape[0]):
    j = list(Y_usable_test[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,num_classes):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])


plot_confusion_matrix(confnorm, labels=classes)


# Plot confusion matrix
from tensorflow_model_optimization.quantization.keras import vitis_quantize
with vitis_quantize.quantize_scope():
    q_model = tf.keras.models.load_model("resnet_checkpoints/model_2_quantized.h5")
test_Y_hat = q_model.predict(X_usable_test, batch_size=1024)
conf = np.zeros([num_classes,num_classes])
confnorm = np.zeros([num_classes,num_classes])
for i in range(0,X_usable_test.shape[0]):
    j = list(Y_usable_test[i,:]).index(1)
    k = int(np.argmax(test_Y_hat[i,:]))
    conf[j,k] = conf[j,k] + 1
for i in range(0,num_classes):
    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])


plot_confusion_matrix(confnorm, labels=classes)





batchsize = 128
progress = ProgressBar()
snrlist = np.unique(Z_test)
acc_snr_arr = []

q_model = tf.keras.models.load_model("resnet_checkpoints/model_0_quantized.h5")

# interate over SNRs
for snr in progress(snrlist):
    acc_arr = []
    i_SNR = np.where(Z_test==snr)
    X_SNR = X_test[i_SNR[0],:,:]
    Y_SNR = Y_test[i_SNR[0],:]
    X_SNR_len = np.shape(X_SNR)[0]
    total_batches = int(X_SNR_len/batchsize)
    
    for i in (range(0, total_batches)):
        x_batch, y_batch = X_SNR[i*batchsize:i*batchsize+batchsize], Y_SNR[i*batchsize:i*batchsize+batchsize]
        
        # model prediction
        pred = q_model.predict(x_batch)
        
        #Pediction values are 0-24, corresponding to indices representing different modulation types
        pred_ind = np.argmax(pred, axis=1)
        expected_ind = np.argmax(y_batch, axis=1)
        matches  = sum(np.equal(pred_ind, expected_ind))
        acc      = matches/batchsize
        acc_arr.append(acc)

    # Average the per-batch accuracy values
    accuracy = np.mean(acc_arr)
    acc_snr_arr.append(accuracy)
    print("SNR: ", snr, "accuracy", accuracy)


plt.figure(figsize=(1,1))
plt.show()
fig= plt.figure(figsize=(10,8))
plt.plot(snrlist, acc_snr_arr, 'bo-', label='accuracy')
plt.ylabel('Accuracy')
plt.xlabel('SNR')
plt.title("Accuracy vs, SNR for INT8 Model")
plt.legend()
plt.axis([-22, 32, 0, 1.0])
plt.grid()


batchsize = 128
progress = ProgressBar()
snrlist = np.unique(Z_test)
acc_snr_arr = []

q_model = tf.keras.models.load_model("resnet_checkpoints/model_1_relu_100ep_fftv2.h5")

# interate over SNRs
for snr in progress(snrlist):
    acc_arr = []
    i_SNR = np.where(Z_test==snr)
    X_SNR = X_test[i_SNR[0],:,:]
    Y_SNR = Y_test[i_SNR[0],:]
    X_SNR_len = np.shape(X_SNR)[0]
    total_batches = int(X_SNR_len/batchsize)
    
    for i in (range(0, total_batches)):
        x_batch, y_batch = X_SNR[i*batchsize:i*batchsize+batchsize], Y_SNR[i*batchsize:i*batchsize+batchsize]
        
        # model prediction
        pred = q_model.predict(x_batch)
        
        #Pediction values are 0-24, corresponding to indices representing different modulation types
        pred_ind = np.argmax(pred, axis=1)
        expected_ind = np.argmax(y_batch, axis=1)
        matches  = sum(np.equal(pred_ind, expected_ind))
        acc      = matches/batchsize
        acc_arr.append(acc)

    # Average the per-batch accuracy values
    accuracy = np.mean(acc_arr)
    acc_snr_arr.append(accuracy)
    print("SNR: ", snr, "accuracy", accuracy)


plt.figure(figsize=(1,1))
plt.show()
fig= plt.figure(figsize=(10,8))
plt.plot(snrlist, acc_snr_arr, 'bo-', label='accuracy')
plt.ylabel('Accuracy')
plt.xlabel('SNR')
plt.title("Accuracy vs, SNR for INT8 Model")
plt.legend()
plt.axis([-22, 32, 0, 1.0])
plt.grid()


batchsize = 128
progress = ProgressBar()
snrlist = np.unique(Z_test)
acc_snr_arr = []

q_model = tf.keras.models.load_model("resnet_checkpoints/model_2_quantized.h5")

# interate over SNRs
for snr in progress(snrlist):
    acc_arr = []
    i_SNR = np.where(Z_test==snr)
    X_SNR = X_test[i_SNR[0],:,:]
    Y_SNR = Y_test[i_SNR[0],:]
    X_SNR_len = np.shape(X_SNR)[0]
    total_batches = int(X_SNR_len/batchsize)
    
    for i in (range(0, total_batches)):
        x_batch, y_batch = X_SNR[i*batchsize:i*batchsize+batchsize], Y_SNR[i*batchsize:i*batchsize+batchsize]
        
        # model prediction
        pred = q_model.predict(x_batch)
        
        #Pediction values are 0-24, corresponding to indices representing different modulation types
        pred_ind = np.argmax(pred, axis=1)
        expected_ind = np.argmax(y_batch, axis=1)
        matches  = sum(np.equal(pred_ind, expected_ind))
        acc      = matches/batchsize
        acc_arr.append(acc)

    # Average the per-batch accuracy values
    accuracy = np.mean(acc_arr)
    acc_snr_arr.append(accuracy)
    print("SNR: ", snr, "accuracy", accuracy)


plt.figure(figsize=(1,1))
plt.show()
fig= plt.figure(figsize=(10,8))
plt.plot(snrlist, acc_snr_arr, 'bo-', label='accuracy')
plt.ylabel('Accuracy')
plt.xlabel('SNR')
plt.title("Accuracy vs, SNR for INT8 Model")
plt.legend()
plt.axis([-22, 32, 0, 1.0])
plt.grid()








### Callback
checkpoint_dir = 'resnet_checkpoints'
#os.mkdir(checkpoint_dir)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_dir + '/model_baseline_qat.h5', 
                                                 verbose = 1,
                                                 save_best_only=True, 
                                                 save_weights_only=False,
                                                 mode='auto')
# tb_callback = TensorBoard(log_dir=os.path.join('learner_logs', 'model_0'),
#                           histogram_freq=1,
#                           profile_batch=0)

callbacks = [cp_callback]


model = tf.keras.models.load_model('resnet_checkpoints/model_baseline.h5')


# *Call Vai_q_tensorflow2 api to create the quantize training model
from tensorflow_model_optimization.quantization.keras import vitis_quantize
quantizer = vitis_quantize.VitisQuantizer(model)
qat_model = quantizer.get_qat_model(
    init_quant=True, # Do init PTQ quantization will help us to get a better initial state for the quantizers, especially for the  `pof2s_tqt` strategy. Must be used together with calib_dataset
    calib_dataset=X_test[1:10000])

# Then run the training process with this qat_model to get the quantize finetuned model.
# Compile the model
qat_model.compile(
        optimizer= Adam(learning_rate=0.0001), 
        loss=tf.keras.losses.CategoricalCrossentropy(),
        metrics=keras.metrics.Accuracy())


# Start the training/finetuning
qat_model.fit(
    X_train,
    Y_train,
    epochs=nb_epoch,
    validation_data=(X_val, Y_val),
    verbose=1,
    callbacks=callbacks)







# Select HW Target Choose Either Quantized or QAT (Fine Tuned Model)

#For kv260
#!vai_c_tensorflow2 -m /workspace/files/quantize_results/quantized_model.h5 -a /opt/vitis_ai/compiler/arch/DPUCZDX8G/KV260/arch.json -o vai_c_output -n rfClassification

#For ZCU104
!vai_c_tensorflow2 -m resnet_checkpoints/best_keras_model_quantized.h5 -a /opt/vitis_ai/compiler/arch/DPUCZDX8G/ZCU104/arch.json -o vai_c_output -n rfClassification

#For ZCU102 
#!vai_c_tensorflow2 -m /workspace/files/quantize_results/quantized_model.h5 -a /opt/vitis_ai/compiler/arch/DPUCZDX8G/ZCU102/arch.json -o vai_c_output -n rfClassification --options "{'cpu_arch':'arm64', 'mode':'normal', 'save_kernel':''}"

#For Alveo U50
#!vai_c_tensorflow2 -m /workspace/files/quantize_results/quantized_model.h5 -a /opt/vitis_ai/compiler/arch/DPUCAHX8H/U50/arch.json -o vai_c_output -n rfClassification --options "{'cpu_arch':'arm64', 'mode':'normal', 'save_kernel':''}"

#For Alveo U50
#!vai_c_tensorflow2 -m /workspace/files/quantize_results/quantized_model.h5 -a /opt/vitis_ai/compiler/arch/DPUCAHX8H/U50/arch.json -o vai_c_output -n rfClassification --options "{'cpu_arch':'arm64', 'mode':'normal', 'save_kernel':''}"

#For Alveo U250
#!vai_c_tensorflow2 -m /workspace/files/quantize_results/quantized_model.h5 -a /opt/vitis_ai/compiler/arch/DPUCADF8H/U250/arch.json -o vai_c_output -n rfClassification --options "{'cpu_arch':'arm64', 'mode':'normal', 'save_kernel':''}"

#For Versal VCK190
#!vai_c_tensorflow2 -m /workspace/files/quantize_results/quantized_model.h5 -a /opt/vitis_ai/compiler/arch/DPUCVDX8G/VCK190/arch.json -o vai_c_output -n rfClassification --options "{'cpu_arch':'arm64', 'mode':'normal', 'save_kernel':''}"


q_model.inputs





!xir png /workspace/files/vai_c_output/rfClassification.xmodel xmodel.png





np.save('/workspace/rf_input.npy', X_test[0:1000,:,:])
np.save('/workspace/rf_classes.npy', Y_test[0:1000])
np.save('/workspace/rf_snrs.npy', Z_test[0:1000])



